{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2021) - Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling policy evaluation as a prediction problem (for all $s \\in S$), \n",
    "$$ q_{\\pi} (s, a) = \\displaystyle\\sum_{s', r} p(s', r|s,a) [r + \\gamma v_{\\pi} (s')] $$\n",
    "$$ v_{\\pi} (s) = \\displaystyle\\sum_{a} \\pi (a|s) \\displaystyle\\sum_{s', r} p(s', r|s,a) [r + \\gamma v_{\\pi} (s')] $$\n",
    "1st iteration:\n",
    "$$ q_1 (s_1, a_1) = (8+10) \\times 0.2 + (8+1) \\times 0.6 + (8+0) \\times 0.2 = 10.6 $$\n",
    "$$ q_1 (s_1, a_2) = (10+10) \\times 0.1 + (10+1) \\times 0.2 + (10+0) \\times 0.7 = 11.2 $$\n",
    "$$ q_1 (s_2, a_1) = (1+10) \\times 0.3 + (1+1) \\times 0.3 + (1+0) \\times 0.4 = 4.3 $$\n",
    "$$ q_1 (s_2, a_2) = (-1+10) \\times 0.5 + (-1+1) \\times 0.3 + (-1+0) \\times 0.2 = 4.3 $$\n",
    "Assuming equiprobable random policy, \n",
    "$$ v_1 (s_1) = 0.5 \\times 10.6 + 0.5 \\times 11.2 = 10.9 $$\n",
    "$$ v_1 (s_2) = 0.5 \\times 4.3 + 0.5 \\times 4.3 = 4.3 $$\n",
    "$$ v_1 (s_3) = 0 $$\n",
    "2nd iteration:\n",
    "$$ q_2 (s_1, a_1) = (8+10.9) \\times 0.2 + (8+4.3) \\times 0.6 + (8+0) \\times 0.2 = 12.76 $$\n",
    "$$ q_2 (s_1, a_2) = (10+10.9) \\times 0.1 + (10+4.3) \\times 0.2 + (10+0) \\times 0.7 = 11.95 $$\n",
    "$$ q_2 (s_2, a_1) = (1+10.9) \\times 0.3 + (1+4.3) \\times 0.3 + (1+0) \\times 0.4 = 5.56 $$\n",
    "$$ q_2 (s_2, a_2) = (-1+10.9) \\times 0.5 + (-1+4.3) \\times 0.3 + (-1+0) \\times 0.2 = 5.74 $$\n",
    "Assuming equiprobable random policy, \n",
    "$$ v_2 (s_1) = 0.5 \\times 12.76 + 0.5 \\times 11.95 = 12.355 $$\n",
    "$$ v_2 (s_2) = 0.5 \\times 5.56 + 0.5 \\times 5.74 = 5.66 $$\n",
    "$$ v_2 (s_3) = 0 $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the greedy policy,\n",
    "$$\\pi' (s) = \\underset{a}{\\operatorname{argmax}} \\displaystyle\\sum_{s',r} p(s',r|s,a) [r+\\gamma v_{\\pi} (s')]$$\n",
    "In this question, after the 1st iteration:\n",
    "$$\\pi_{a_2} (s_1) = 11.2 $$\n",
    "$$\\pi_{a_1, a_2} (s_2) = 4.3 $$\n",
    "After the 2nd iteration:\n",
    "$$\\pi_{a_1} (s_1) = 12.76 $$\n",
    "$$\\pi_{a_2} (s_2) = 5.74 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimal deterministic policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the optimal deterministic policy, I compute one more iteration and the result is as below (after the 3rd iteration):\n",
    "$$ q_3 (s_1, a_1) = (8+12.355) \\times 0.2 + (8+5.66) \\times 0.6 + (8+0) \\times 0.2 = 13.875 $$\n",
    "$$ q_3 (s_1, a_2) = (10+12.355) \\times 0.1 + (10+5.66) \\times 0.2 + (10+0) \\times 0.7 = 12.3675 $$\n",
    "$$ q_3 (s_2, a_1) = (1+12.355) \\times 0.3 + (1+5.66) \\times 0.3 + (1+0) \\times 0.4 = 6.4045 $$\n",
    "$$ q_3 (s_2, a_2) = (-1+12.355) \\times 0.5 + (-1+5.66) \\times 0.3 + (-1+0) \\times 0.2 = 6.8755 $$\n",
    "$$ v_3 (s_1) = 0.5 \\times 13.875 + 0.5 \\times 12.3675 = 13.1213 $$\n",
    "$$ v_3 (s_2) = 0.5 \\times 6.4045 + 0.5 \\times 6.8755 = 6.64 $$\n",
    "$$ v_3 (s_3) = 0 $$  \n",
    "$$\\pi_{a_1} (s_1) = 13.875 $$\n",
    "$$\\pi_{a_2} (s_2) = 6.8755 $$\n",
    "Thus, we can argue that the optimal deterministic policy could be: 1). taking action $a_1$ once we see state $s_1$, while 2). taking action $a_2$ once we see state $s_2$. The main reason is the monotonicity property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Complexity of each value iteration is $O(as^2)$, while $O(as^2 + s^3)$ for poicy iteration.\n",
    "* For value iteration, the # of iteration may grow exponentially if the discount factor approaches 1 and faster if the transition function is sparse. Thus, policy iteration is usually faster than value iteration becaus ea policy converges more quickly than a value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job-Hopping and Wages-Utility-Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* State space:\n",
    "$S = (i, s)$ where $1 \\leq i \\leq n$, $s \\in \\{s_1, s_2\\}$, and $s_1$ indicates the worker being employed while $s_2$ unemployed.\n",
    "* Action space: \n",
    "$A (i,s) = \\begin{gather*}\n",
    "\\begin{cases}\n",
    "  None  & 1 \\leq i \\leq n, s = s_1\\\\    \n",
    "  \\{a_1,a_2\\}  & 1 \\leq i \\leq n, s = s_2\n",
    "\\end{cases}\n",
    "\\end{gather*}$ where $a_1$ indicates accepting the offered job while $s_2$ declining the offered job.\n",
    "* Transition function:\n",
    "$Pr[(i',s')|(i,s),a_1] = \\begin{gather*}\n",
    "\\begin{cases}\n",
    "  \\alpha p_{i'}   & 1 \\leq i' \\leq n, s \\in \\{s_1, s_2\\}, s' =s_1\\\\   \n",
    "  1-\\alpha  & i' = i, s \\in \\{s_1, s_2\\}, s' = s_1\\\\\n",
    "  0   & otherwise\n",
    "\\end{cases}\n",
    "\\end{gather*}$\n",
    "$Pr[(i',s')|(i,s),a_2] = \\begin{gather*}\n",
    "\\begin{cases}\n",
    "  p_{i'}   & 1 \\leq i' \\leq n, s = s_2, s' =s_2\\\\ \n",
    "  0  & otherwise\n",
    "\\end{cases}\n",
    "\\end{gather*}$\n",
    "* Reward function:   \n",
    "    - $R((i,s), a_1) = U(w_i)$, $1 \\leq i \\leq n$, $s \\in \\{ s_1, s_2\\}$\n",
    "    - $R((i,s), a_2) = U(w_0)$, $1 \\leq i \\leq n$\n",
    "* Bellman optimality equation (we start from a given day t)\n",
    "    - Optimal Value function($0 \\leq iu \\leq n$, $u \\geq t$): \n",
    "        - $V_{it} = \\displaystyle\\sum_{iu=it}^{n} p_{iu} max(log(w_{it}) + \\gamma V_{it}, V_{iu}) $\n",
    "        - $V_{iu} = log(w_{iu}) + \\gamma (\\alpha V_{it} + (1-\\alpha) V_{iu})$\n",
    "    - Optimal policy($0 \\leq iu \\leq n$, $u \\geq t$):\n",
    "        - The worker should accpet the offered job if and only if $V_{iu} > log(w_{it}) + \\gamma V_{it}$ to maximize the total expected rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DP (numeric iterative algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_trans: Sequence[float] = [0.1, 0.2, 0.2, 0.3, 0.2]\n",
    "trial_wages: Sequence[float] = [1.0, 1.2, 1.7, 3.5, 4.8, 6.3]\n",
    "trial_gamma: float = 0.75\n",
    "trial_alpha: float = 0.12\n",
    "trial_risk_aversion: float = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WageUtilityMax:\n",
    "    \n",
    "    trans: Sequence[float]\n",
    "    wages: Sequence[float]\n",
    "    gamma: float\n",
    "    alpha: float\n",
    "    risk_aversion: float\n",
    "    \n",
    "    def wages_utility(self) -> Sequence[float]:\n",
    "        a = self.risk_aversion\n",
    "        lst = []\n",
    "        for w in self.wages:\n",
    "            if a != 1:\n",
    "                lst.append((pow(w, 1-a)-1)/(1-a))\n",
    "            else:\n",
    "                lst.append(np.log(w))\n",
    "        return lst\n",
    "    \n",
    "    def optimal_value_func(self) -> Sequence[float]:\n",
    "        iters = len(self.trans)\n",
    "        utilities = self.wages_utility()\n",
    "        optimal_value_function = np.zeros(iters+1)\n",
    "        tolerance = 1e-6\n",
    "        epsilon = tolerance * 1e6\n",
    "        while epsilon >= tolerance:\n",
    "            old_optimal_value_function = [v for v in optimal_value_function]\n",
    "            optimal_value_function[0] = sum(self.trans[i] *\\\n",
    "                                            max(optimal_value_function[i+1], utilities[0] +\\\n",
    "                                                self.gamma * optimal_value_function[0]\\\n",
    "                                               ) for i in range(iters))\n",
    "            for i in range(1, iters+1):\n",
    "                optimal_value_function[i] = utilities[i] + self.gamma *(self.alpha *\\\n",
    "                                                                        optimal_value_function[0] +\\\n",
    "                                                                        (1 - self.alpha) * optimal_value_function[i])\n",
    "            epsilon = max(abs(old_optimal_value_function[i] - v\\\n",
    "                             ) for i, v in enumerate(optimal_value_function))\n",
    "        return optimal_value_function\n",
    "\n",
    "    def optimal_policy(self) -> Sequence[str]:\n",
    "        iters = len(self.trans)\n",
    "        utilities = self.wages_utility()\n",
    "        optimal_value_function = self.optimal_value_func()\n",
    "        for i in range(1, iters+1):\n",
    "            if optimal_value_function[i] > utilities[0] + self.gamma * optimal_value_function[0]:\n",
    "                print(\"policy update\")\n",
    "            else:\n",
    "                print(\"decline policy update\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decline policy update\n",
      "decline policy update\n",
      "policy update\n",
      "policy update\n",
      "policy update\n",
      "[5.88821679 2.09980243 3.16146466 5.48398494 6.55375278 7.50222936]\n"
     ]
    }
   ],
   "source": [
    "trial_wage_utility = WageUtilityMax(\n",
    "    trans=trial_trans,\n",
    "    wages=trial_wages,\n",
    "    gamma=trial_gamma,\n",
    "    alpha=trial_alpha,\n",
    "    risk_aversion=trial_risk_aversion)\n",
    "\n",
    "optimal_value_function = trial_wage_utility.optimal_value_func()\n",
    "optimal_policy = trial_wage_utility.optimal_policy()\n",
    "print(optimal_value_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
