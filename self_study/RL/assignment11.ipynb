{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2021) - Assignment 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gridWorldEnvironment import GridWorld\n",
    "gw = GridWorld(gamma = .9, theta = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, -1), (6, -1), (7, -1), (7, -1), (3, -1), (3, -1), (3, -1), (3, -1), (3, -1), (3, -1), (3, -1), (7, -1), (7, -1), (7, -1), (11, -1), (11, -1), (7, -1), (6, -1), (5, -1), (4, -1), (4, -1), (0, -1)]\n"
     ]
    }
   ],
   "source": [
    "def generate_random_episode(env):\n",
    "    episode = []\n",
    "    done = False\n",
    "    current_state = np.random.choice(env.states)\n",
    "    episode.append((current_state, -1))\n",
    "    while not done:\n",
    "        action = np.random.choice(env.actions)\n",
    "        next_state, reward = gw.state_transition(current_state, action)\n",
    "        episode.append((next_state, reward))\n",
    "        if next_state == 0:\n",
    "            done = True\n",
    "        current_state = next_state\n",
    "    return episode\n",
    "\n",
    "print(generate_random_episode(gw))\n",
    "\n",
    "# create and initialize value array (include 2 terminal states)\n",
    "def value_array(env):\n",
    "    return np.zeros(len(env.states)+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-visit MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(env, num_iter):\n",
    "    values = value_array(env)\n",
    "    returns = dict()\n",
    "    for state in env.states:\n",
    "        returns[state] = list()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        episode = generate_random_episode(env)\n",
    "        # use 'already_visited' as the variable to exclude states we have visited\n",
    "        # initialize with the terminal state in it\n",
    "        already_visited = set({0})   \n",
    "        for s, r in episode:\n",
    "            if s not in already_visited:\n",
    "                already_visited.add(s)\n",
    "                idx = episode.index((s, r))\n",
    "                G = 0\n",
    "                j = 1\n",
    "                # walk through all states in the random generated episode in each iteration\n",
    "                while j + idx < len(episode):\n",
    "                    G = env.gamma * (G + episode[j + idx][1])\n",
    "                    j += 1\n",
    "                returns[s].append(G)\n",
    "                # we only visit each state once; values[s] = returns[s]\n",
    "                values[s] = np.mean(returns[s])\n",
    "    return values, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -4.77835972, -6.40212876, -6.86471444, -4.77615712,\n",
       "       -5.95115629, -6.43682686, -6.40968011, -6.38685514, -6.46450301,\n",
       "       -5.90752721, -4.70317018, -6.89330328, -6.44994181, -4.74261567,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, returns = first_visit_mc(gw, 10000)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every-visit MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_mc(env, num_iter):\n",
    "    values = value_array(env)\n",
    "    returns = dict()\n",
    "    for state in env.states:\n",
    "        returns[state] = list()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        episode = generate_random_episode(env)\n",
    "        for s, r in episode:\n",
    "            if s != 0: # a simple judgement, we don't need to make a set here\n",
    "                idx = episode.index((s, r))\n",
    "                G = 0\n",
    "                j = 1\n",
    "                while j + idx < len(episode):\n",
    "                    G = env.gamma * (G + episode[j + idx][1])\n",
    "                    j += 1\n",
    "                returns[s].append(G)\n",
    "                # we assume f(n) = 1/n; see 2/17 - RL Slide 10/44\n",
    "                values[s] = np.mean(returns[s])\n",
    "    return values, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -5.83560098, -7.44156575, -7.75810965, -5.88156501,\n",
       "       -7.06147093, -7.52677057, -7.48542171, -7.47299368, -7.52990292,\n",
       "       -7.08556877, -5.95335775, -7.81574431, -7.51473732, -5.97236212,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, returns = every_visit_mc(gw, 10000)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular TD Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate policy and episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy(env):\n",
    "    random_policy = {}\n",
    "    for state in env.states:\n",
    "        random_num = sorted(np.random.sample(3))\n",
    "        actions = env.actions\n",
    "        prob = [random_num[0], random_num[1] - random_num[0], random_num[2] - random_num[1], 1-random_num[2]]\n",
    "        random_policy[state] = (actions, prob)\n",
    "    return random_policy\n",
    "\n",
    "def generate_episode(env, s0, a0, policy):\n",
    "    episode = []\n",
    "    done = False\n",
    "    current_state, action = s0, a0\n",
    "    episode.append((current_state, action, -1))\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward = gw.state_transition(current_state, action)\n",
    "        prob = policy[current_state][1]\n",
    "        prob[np.argmax(prob)] -= .2\n",
    "        prob[np.random.choice(np.delete(np.arange(4), np.argmax(prob)))] += .1\n",
    "        prob[np.random.choice(np.delete(np.arange(4), np.argmax(prob)))] += .05\n",
    "        prob[np.random.choice(np.delete(np.arange(4), np.argmax(prob)))] += .05\n",
    "\n",
    "        action = np.random.choice(policy[current_state][0], p = prob)\n",
    "        episode.append((next_state, action, reward))\n",
    "        \n",
    "        if next_state == 0:   \n",
    "            done = True\n",
    "        current_state = next_state\n",
    "    return episode[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_td(env, alpha, num_iter):\n",
    "    value_fuc = np.zeros(len(env.states)+2)\n",
    "    pi = generate_policy(env)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        s0, a0 = np.random.choice(env.states), np.random.choice(env.actions)\n",
    "        episode = generate_episode(env, s0, a0, pi)\n",
    "        already_visited = set()\n",
    "        \n",
    "        for step in range(len(episode)):\n",
    "            current_state, action = episode[step][0], episode[step][1]\n",
    "            next_state, reward = env.state_transition(current_state, action)\n",
    "            value_fuc[current_state] += alpha * (reward + env.gamma * value_fuc[next_state] - value_fuc[current_state])            \n",
    "    return value_fuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -7.56807923, -7.77505403, -8.11106428, -4.60630788,\n",
       "       -8.04859408, -7.33351423, -7.81881166, -8.24851365, -7.8158604 ,\n",
       "       -7.29291106, -3.56236681, -8.23071182, -7.54831316, -6.12672111,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = tabular_td(gw, .5, 1000)\n",
    "values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
