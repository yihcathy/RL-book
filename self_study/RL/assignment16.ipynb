{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2021) - Assignment 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo (REINFORCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0): \n",
    "        \n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        \n",
    "        for t in itertools.count():      \n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            values.episode_rewards[i_episode] += reward\n",
    "            values.episode_lengths[i_episode] = t\n",
    "\n",
    "            if done:\n",
    "                break             \n",
    "            state = next_state\n",
    "    \n",
    "        for t, transition in enumerate(episode):\n",
    "            total_return = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "            baseline_value = estimator_value.predict(transition.state)            \n",
    "            advantage = total_return - baseline_value\n",
    "            estimator_value.update(transition.state, total_return)\n",
    "            estimator_policy.update(transition.state, advantage, transition.action)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTOR-CRITIC-ELIGIBILITY-TRACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()     \n",
    "        episode = []      \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            values.episode_rewards[i_episode] += reward\n",
    "            values.episode_lengths[i_episode] = t\n",
    "            \n",
    "            value_next = estimator_value.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - estimator_value.predict(state)\n",
    "            \n",
    "            estimator_value.update(state, td_target)\n",
    "            estimator_policy.update(state, td_error, action)\n",
    "\n",
    "            if done:\n",
    "                break              \n",
    "            state = next_state\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\begin{align*}\n",
    "    log_{\\pi} (s,a; \\theta) &= \\theta^T \\phi(s,a) - log(\\displaystyle\\sum_{b \\in A} e^{\\theta^T \\phi(s,b)}) \\\\\n",
    "    \\frac{d}{d \\theta_i}(log_{\\pi} (s,a; \\theta)) &= \\phi_i (s,a) - \\frac{\\displaystyle\\sum_{b \\in A} \\phi_i (s,b) e^{\\theta^T \\phi(s,b)}}{\\displaystyle\\sum_{b \\in A} e^{\\theta^T \\phi(s,b)}} \\\\\n",
    "    &= \\phi_i (s,a) - \\displaystyle\\sum_{b \\in A} \\pi (b,s;\\theta) \\theta_i (s,b) \\\\\n",
    "    &= \\phi_i (s,a) - \\mathbb{E}_{\\pi} [\\theta_i (s,\\cdot)] \\\\\n",
    "  \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the Action-Value function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q (s,a;w) = w^T \\frac{d}{d \\theta}(log_{\\pi} (s,a; \\theta)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-Value function approximation has zero mean for any state s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\begin{align*}\n",
    "    \\displaystyle\\sum_{a \\in A} \\pi(s,a;\\theta) Q(s,a;w) &= \\displaystyle\\sum_{a \\in A} \\pi(s,a;\\theta) w^T \\frac{d}{d \\theta}(log_{\\pi} (s,a; \\theta)) \\\\\n",
    "    &= \\displaystyle\\sum_{a \\in A} w^T \\frac{d}{d \\theta}(log_{\\pi} (s,a; \\theta)) \\\\\n",
    "    &= w^T \\frac{d}{d\\theta} \\displaystyle\\sum_{a \\in A} \\pi(s,a;\\theta) \\\\\n",
    "    &= 0 \\\\\n",
    "  \\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
