{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2021) - Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4  MDP Bellman Policy Equations for a Deterministric Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming a deterministric policy $\\pi_D: S \\rightarrow A$, for all $s \\in N$:\n",
    "<br> $$V^{\\pi_D} (s) = R^{\\pi_D} (s) + \\gamma \\displaystyle\\sum_{s' \\in N} P^{\\pi_D} (s, s') V^{\\pi_D} (s') $$ <div style=\"text-align: right\"> (1) </div>\n",
    "<br> $$V^{\\pi_D} (s) = Q^{\\pi_D} (s, a) $$ <div style=\"text-align: right\"> (2) </div>\n",
    "<br> $$Q^{\\pi_D} (s, a) = R(s,a) + \\gamma \\displaystyle\\sum_{s' \\in N} P(s, a, s') V^{\\pi_D} (s') $$ <div style=\"text-align: right\"> (3) </div>\n",
    "<br> Considering (2) and (3),\n",
    "<br> $$Q^{\\pi_D} (s, a) = R(s,a) + \\gamma \\displaystyle\\sum_{s' \\in N} P(s, a, s') Q^{\\pi_D} (s', a)$$ <div style=\"text-align: right\"> (4) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of MDP Bellman Optimality Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimal Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With reference to the MDP State-Value Function Bellman Optimality Equation, $V^{*} (s) = max_{a \\in A} \\{R(s, a) + \\gamma \\displaystyle\\sum_{s' \\in N} P(s, a, s') V^{*}(s')\\}$\n",
    "<br> For all $s \\in S $, the optimial value function in this question can be derived as: \n",
    "<br> $$V^{*} (s) = max_{a \\in [0,1]} \\{a \\times [(1-a) + 0.5 V^{*} (s')] + (1-a) \\times [(1+a) + 0.5 V^{*} (s')] \\} $$ \n",
    "<br> Since each state in the question has the same values for transition probability and reward function, we can use $V^{*}$ to directly substitute for $V^{*} (s)$ and $V^{*} (s')$.\n",
    "<br> Thus, $$ V^{*} = max_{a \\in [0,1]} \\{a \\times [(1-a) + 0.5 V^{*}] + (1-a) \\times [(1+a) + 0.5 V^{*}] \\} $$\n",
    "<br> $\\implies$ $$ V^{*} = max_{a \\in [0,1]} \\{-2a^{2} +a + 1\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimal Deterministic Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $a \\in [0,1]$, the value function is maximized ($V^{*} = 1 $) when $\\pi ^{*} (s) = a = 0$ for all $s \\in S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frog-escape Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* State space: $$S = \\{i | 0 \\leq i \\leq n\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Action space: $$ A = \\{ A, B\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transitions function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $1 \\leq i \\leq n-1 $:\n",
    "<br> $$P [i'| ( i, A)] = \\begin{gather*}\n",
    "\\begin{cases}\n",
    "  \\frac{i}{n}  & i' = i - 1\\\\    \n",
    "  \\frac{n-1}{n}  & i' = i + 1\\\\  \n",
    "  0  & otherwise\n",
    "\\end{cases}\n",
    "\\end{gather*}$$\n",
    "<br>  $$P [i'| ( i, B)] = \\begin{gather*}\n",
    "\\begin{cases}\n",
    "  \\frac{1}{n}  & 0 \\leq i' \\leq n, i' \\neq i \\\\    \n",
    "  0  & i' = i\n",
    "\\end{cases}\n",
    "\\end{gather*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rewards function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $1 \\leq i \\leq n-1$ and $a \\in \\{ A, B\\}$:\n",
    "<br> $$R [i'| ( i, a)] = \\begin{gather*}\n",
    "\\begin{cases}\n",
    "  1  & i' = n \\\\    \n",
    "  0  & otherwise\n",
    "\\end{cases}\n",
    "\\end{gather*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lilypads_mdp(n: int):\n",
    "    mdp = {0: {'crock_A': {0: (1., 0.)}, 'crock_B': {0: (1., 0.)}}}\n",
    "    for i in range(1,n):\n",
    "        mdp_i = {i: {'crock_A': {i-1: (i/n, 0.), i+1: (1.-i/n,1. if i == n-1 else 0.)},\\\n",
    "                    'crock_B': {j: (1/n, 1. if j == n else 0.) for j in range(n+1) if j != i}}}\n",
    "        mdp.update(mdp_i)\n",
    "    mdp_n = {n: {'crock_A': {n: (1., 0.)}, 'crock_B': {n: (1., 0.)}}}\n",
    "    mdp.update(mdp_n)\n",
    "    return mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lilypads_finite_policy(n: int) -> Mapping[int, float]:\n",
    "    value_function = [0.5] * (n+1)\n",
    "    value_function[0] = 0.\n",
    "    value_function[n] = 0.\n",
    "    tolerance = 1e-8\n",
    "    epsilon = tolerance * 1e4\n",
    "    while epsilon >= tolerance:\n",
    "        old_value_function = [v for v in value_function]\n",
    "        for i in range(1, n):\n",
    "            value_function[i] = max((1. if i == n-1 else 0.) + i * value_function[i-1] + (n-1) * value_function[i+1],\\\n",
    "                                    1.+sum(value_function[j] for j in range(1,n) if j != i))/n\n",
    "        epsilon = max(abs(old_value_function[i]-v) for i,v in enumerate(value_function))\n",
    "    return {v:f for v,f in enumerate(value_function)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP with Normal Distribution Transition (Î³ = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal cost in this question can be expressed as the following optimal value function:\n",
    "<br> $$ V^{*} (s) = max_{a \\in \\mathbb{R}} \\{ \\mathbb{E}_{s'\\sim N(s, \\sigma^{2})} (-e^{as'})\\}$$\n",
    "<br> $$ = max_{a \\in \\mathbb{R}} \\{ \\int\\limits_{-\\infty}^{+\\infty} \\frac{-1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-(s+a\\sigma^2))^2}{2\\sigma_2}} e^{sa + \\frac{\\sigma^2 a^2}{2}}dx\\}$$\n",
    "<br> $$ = max_{a \\in \\mathbb{R}} \\{-e^{sa + \\frac{\\sigma^2 a^2}{2}} \\mathbb{E}_{x \\sim N(s+a\\sigma^2, \\sigma^2)} 1\\} $$\n",
    "<br>$$ = max_{a \\in \\mathbb{R}} \\{-e^{sa + \\frac{\\sigma^2 a^2}{2}} \\}$$\n",
    "<br>The value function is maximized when $$ \\frac{\\mathrm d}{\\mathrm d a} \\big( -e^{sa + \\frac{\\sigma^2 a^2}{2}} \\big) = 0$$\n",
    "<br> $$a=\\frac{-s}{\\sigma^2}$$\n",
    "<br> In this case: $$V^{*} (s)= - e^{-\\frac{s^2}{2\\sigma^2}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
