{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2021) - Assignment 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gridWorldEnvironment import GridWorld\n",
    "gw = GridWorld(gamma = .9, theta = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Step TD Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_policy(env):\n",
    "    random_policy = {}\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for action in env.actions:\n",
    "            actions.append(action)\n",
    "            prob.append(0.25)\n",
    "        random_policy[state] = (actions, prob)\n",
    "    return random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_td_prediction(env, alpha = .5, n = 3, num_iter = 100):\n",
    "    value_func = {}\n",
    "    for state in env.states:\n",
    "        value_func[state] = np.random.normal()     \n",
    "    policy = generate_random_policy(env)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        state_trace = []\n",
    "        action_trace = []\n",
    "        reward_trace = []\n",
    "        current_state = np.random.choice(env.states)\n",
    "        state_trace.append(current_state)\n",
    "        trace, total_trace = 0, 10000\n",
    "        while True:\n",
    "            if trace < total_trace:\n",
    "                action = np.random.choice(policy[current_state][0], p = policy[current_state][1])\n",
    "                action_trace.append(action)\n",
    "                next_state, reward = env.state_transition(current_state, action)\n",
    "                state_trace.append(next_state)\n",
    "                reward_trace.append(reward)\n",
    "                if next_state == 0:\n",
    "                    total_trace = trace + 1\n",
    "                    \n",
    "            step = trace - n + 1   \n",
    "            if step >= 0:               \n",
    "                returns = 0\n",
    "                for j in range(step+1, min([step+n, total_trace])+1):\n",
    "                    returns += (env.gamma ** (j - step - 1)) * reward_trace[j-1]\n",
    "                if step + n < total_trace: \n",
    "                    returns += (env.gamma ** n) * value_func[state_trace[step + n]]\n",
    "                value_func[state_trace[step]] += alpha * (returns - value_func[state_trace[step]])\n",
    "            \n",
    "            if step == (total_trace-1):\n",
    "                break\n",
    "            current_state = next_state\n",
    "            trace += 1\n",
    "    return value_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.89560882, -7.22369492, -8.68058755, -3.61373811, -5.96515275,\n",
       "       -8.63247721, -7.48758891, -4.70309762, -7.41797356, -6.19668666,\n",
       "       -8.26693171, -5.09803511, -7.15102475, -5.69806612])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = n_step_td_prediction(gw, num_iter = 10000)\n",
    "np.array(list(values.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Error & TD errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall two points of TD error: is the error in the estimate made at that time; depends on the next state and next reward and is available till one time step later. TD error is defined as: $$\\delta_t \\doteq R_{t+1} + \\gamma V(S_{t+1}) -V(S_t)$$\n",
    "The Monte Carlo error can be written as:\n",
    "$$\n",
    "  \\begin{align*}\n",
    "    G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\\\\n",
    "    &= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\\n",
    "    &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\ \n",
    "    &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma \\delta_{t+2} + \\cdots + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V(S_T)) \\\\\n",
    "    &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma \\delta_{t+2} + \\cdots + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (0 - 0) \\\\\n",
    "    &= \\displaystyle\\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k \\\\\n",
    "  \\end{align*}\n",
    "$$\n",
    "which shows that the MC Error can be written as the sum of discounted TD errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
