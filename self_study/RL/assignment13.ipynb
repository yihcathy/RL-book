{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2021) - Assignment 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gridWorldEnvironment import GridWorld\n",
    "gw = GridWorld(gamma = .9, theta = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA On-policy TD Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = {}\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = np.random.normal()\n",
    "    for action in env.actions:\n",
    "        q[0., action] = 0.\n",
    "    return q\n",
    "\n",
    "def generate_greedy_policy(env, Q):\n",
    "    policy = {}\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        q_values = []\n",
    "        prob = []  \n",
    "        for a in env.actions:\n",
    "            actions.append(a)\n",
    "            q_values.append(Q[state,a])   \n",
    "        for i in range(len(q_values)):\n",
    "            if i == np.argmax(q_values):\n",
    "                prob.append(1)\n",
    "            else:\n",
    "                prob.append(0)                 \n",
    "        policy[state] = (actions, prob)\n",
    "    return policy\n",
    "\n",
    "def e_greedy(env, e, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    prob = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[(state, action)])\n",
    "    for i in range(len(action_values)):\n",
    "        if i == np.argmax(action_values):\n",
    "            prob.append(1 - e + e/len(action_values))\n",
    "        else:\n",
    "            prob.append(e/len(action_values))\n",
    "    return np.random.choice(actions, p = prob)\n",
    "\n",
    "def greedy(env, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[state, action])\n",
    "    return actions[np.argmax(action_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, epsilon, alpha, num_iter):\n",
    "    Q = state_action_value(env)\n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        current_action = e_greedy(env, epsilon, Q, current_state)\n",
    "        while current_state != 0:\n",
    "            next_state, reward = env.state_transition(current_state, current_action)\n",
    "            next_action = e_greedy(env, epsilon, Q, next_state)\n",
    "            Q[current_state, current_action] += alpha * (reward + env.gamma * Q[next_state, next_action] - Q[current_state, current_action])\n",
    "            current_state, current_action = next_state, next_action\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.90002805, -2.77441382, -3.25421038, -1.        , -3.94494846,\n",
       "       -4.22697587, -3.64832604, -1.90000032, -3.93100271, -2.74871695,\n",
       "       -3.72877216, -3.72013637, -1.        , -3.26589619, -2.74641095,\n",
       "       -1.90160927, -1.90000001, -3.49188953, -4.01193861, -3.28579892,\n",
       "       -2.71866513, -4.07809399, -3.72830488, -3.70822778, -3.81109512,\n",
       "       -2.88221503, -3.90667851, -3.60862454, -1.91441847, -4.29019236,\n",
       "       -3.84262207, -2.84192933, -3.67826806, -3.64673726, -2.89719224,\n",
       "       -3.56404418, -3.66016655, -1.98192945, -2.83877892, -3.60843753,\n",
       "       -3.52313767, -1.        , -2.99683135, -3.09334585, -3.79253324,\n",
       "       -3.92130484, -4.14899919, -3.95698485, -3.96391854, -4.23036559,\n",
       "       -1.91061882, -3.78570823, -2.81181946, -1.9       , -1.        ,\n",
       "       -3.95300302,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = sarsa(gw, 0.1, 0.5, 10000)\n",
    "np.array(list(values.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, epsilon, alpha, num_iter):\n",
    "    Q = state_action_value(env)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        while current_state != 0:\n",
    "            current_action = e_greedy(env, epsilon, Q, current_state)\n",
    "            next_state, reward = env.state_transition(current_state, current_action)\n",
    "            best_action = greedy(env, Q, next_state)\n",
    "            Q[current_state, current_action] += alpha * (reward + env.gamma * Q[next_state, best_action] - Q[current_state, current_action])\n",
    "            current_state = next_state\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9  , -2.71 , -2.71 , -1.   , -2.71 , -3.439, -3.439, -1.9  ,\n",
       "       -3.439, -2.71 , -3.439, -2.71 , -1.   , -2.71 , -2.71 , -1.9  ,\n",
       "       -1.9  , -3.439, -3.439, -1.9  , -2.71 , -2.71 , -2.71 , -2.71 ,\n",
       "       -3.439, -1.9  , -2.71 , -3.439, -1.9  , -3.439, -3.439, -2.71 ,\n",
       "       -2.71 , -2.71 , -2.71 , -2.71 , -3.439, -1.9  , -1.9  , -3.439,\n",
       "       -2.71 , -1.   , -1.9  , -2.71 , -2.71 , -3.439, -2.71 , -3.439,\n",
       "       -3.439, -2.71 , -1.9  , -3.439, -2.71 , -1.9  , -1.   , -2.71 ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = q_learning(gw, 0.2, 1.0, 10000)\n",
    "np.array(list(values.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are three cases in the question, we need to consider them separately for markov devision process modeling.\n",
    "1. In the case of selling the entire quantity of stock purchased in the previous day, the states can be written as $ (t, x_t, l_t, b_t, s_t) $\n",
    "<br> where $ t \\in \\{1,2,3, ...T\\}$ (date), $x_t \\in \\mathbb{R}_{\\geq 0}$ (net cash), $l_t \\in \\mathbb{R}_{\\geq 0}$ (liability), $b_t \\in \\mathbb{R}_{\\geq 0}$ (unfulfilled withdrawal requests), $s_t \\in \\mathbb{R}_{\\geq 0}$ (stock value)\n",
    "    - The key consideration in the problem is that $c_t \\geq c_{min}$ where $c_{min} = K cot(\\frac{\\pi c_{min}}{2C})$\n",
    "2. In the case of deciding to increase/reduce the liability\n",
    "<br> If we let $y_t \\in \\mathbb{R}$ as the amount of change in liability, $y_t$ has the following constraints.\n",
    "    - $y_t \\geq - l_t$\n",
    "    - $ y_t \\geq c_{min} - x_t$\n",
    "<br> Thus, we can conclude that $ y_t \\geq max(-l_t, c_{min} - x_t)$\n",
    "3. In the case of deciding to purchase a certain quantity of stock\n",
    "<br> If we let $z_t \\in \\mathbb{R}_{\\geq 0}$ as the number of stock shares to purchase.\n",
    "    - $ 0 \\leq z_t \\leq \\frac{x_t + y_t - c_{min}}{s_t} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this MDP, action is defined as $ (y_t, z_t) $\n",
    "<br> where $ t < T, y_T = z_T = 0$\n",
    "<br> The state transitions can be defined as\n",
    "$$ x_{t+1} = max(x_t + y_t - z_t s_t - K cot(\\frac{\\pi min(x_t + y_t - z_t s_t, C)}{2C}) + d_{t+1} - f((t_1, b_t),0) $$\n",
    "$$ b_{t+1} = max(-x_t - y_t + z_t s_t + K cot(\\frac{\\pi min(x_t + y_t - z_t s_t, C)}{2C}) - d_{t+1} + f((t_1, b_t),0) $$\n",
    "$$ s_{t+1} = g(t+1, s_t) $$\n",
    "where $d_t \\in \\mathbb{R}_{\\geq 0}$ denotes the deposits on day $t$, $w_t = f(t, b_{t-1}), 2 \\leq t \\leq T$ denotes the withdrawal requests on day $t$, $s_t = g(t, s_{t-1})$ denotes the stock value on day $t$\n",
    "<br> The reawrd on day $t, 1 \\leq t \\leq T-1 $ is 0, while the reward on day $T$ is $U(x_T - l_T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solove this problem with reinforcement learning algorithm, here are some points we need to pay attention to:\n",
    "* Histort data, which includes deposits, withdrawals, daily stock price movements can be used to build the above MDP model. \n",
    "* The state transition probabilities based on the statistically estimated probabilities of deposits $d_t$, withdrawals $f(t, b_{t-1})$ and stock moves $g(t, s_{t-1})$. We can also consider other explanatory factors for deposits, withdrawals and stock moces to make more comprehensive estimates of the statistically estimated probabilities. We may need to predict the future movements of these explanatary factors and use them to predict the probabilities of future deposits, withdrawals, and stock moves.\n",
    "* Then we need to use this MDP model to generate a set of simulation episodes with appropriate sampling from the estimated probability distributions.\n",
    "* The action space in this question is rather large, policy gradient algorithm can be used.\n",
    "* We may need an appropriate function approximation such as Actor-Critic algorithm for the Q-value function for policy gradient. Additionally, we need to pay attenton to the features we choose for both the Actor and the Critic neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Reference: CME 241 2020 Final Exam Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
